Suppose we want to solve a machine learning problem. Our design is a deep neural network. Then our goal is to find the best suitable parameters of the neural network that solves the problem.

Let's write the parameters as $\theta$. We will introduce two different approaches of solving the parameters such that the neural network does what we want.

**Non-Probabilistic Learning**
Our first approach is non-probabilistic learning. We define our neural network as $f_{\theta} : \mathcal{X} \Rightarrow \mathcal{Y}$. Our function $f_{\theta}$ is a function of the family $\mathcal{F} = \{ f_{\theta}(x) = y \vert x \in \mathcal{X}, y \in \mathcal{Y}, \theta \in \Theta \}$,
What this means is that the function depends on the variable $\theta$. And depending on how we choose $\theta$, the function will return different values. So in a way, our function can behave differently depending on $\theta$.
Obviously, it is not the easiest task to find a suitible function $f_{\theta}$ for a given task. If it was, then Deep Learning Theory would be a solved problem. Hence it is up to the engineer to find a good architecture of $f_{\theta}$. For now, let us assume that it's a Deep Neural Network (DNN).
